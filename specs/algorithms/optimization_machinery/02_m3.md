# M3 (Multi-Scale Momentum with CMS)

```
CONTRACT
  Purpose:    CMS applied to the optimizer itself. Standard momentum operates
              at a single timescale. M3 uses k frequency levels, each with
              its own momentum accumulator at a different timescale.
              Fast levels track short-term gradient trends.
              Slow levels track long-term optimization direction.
  Expects:    Gradient signal. k frequency levels with chunk sizes.
              Per-level momentum state. Pulse with active levels.
  Guarantees: Multi-scale gradient history compression.
              Level 0 (fastest) captures per-token patterns.
              Level k-1 (slowest) captures sequence-wide trends.
              Frozen levels accumulate error for later application.
  Cost:       k * O(per_level_momentum_cost). But only active levels compute.
              At most steps, only level 0 is active (cheapest).
  Trade-off:  More levels = finer temporal resolution = more state.
              k=1 recovers standard single-scale momentum.
              k=4 with [1,8,64,512] frequencies gives 4 temporal scales.
  Position:   specs/algorithms/optimization_machinery/02_m3.md
  Source:     HOPE (2512.24695) Section 7, Eq 71 applied to optimizer
```

## Architecture

```
-- Standard momentum: one accumulator, one timescale
S = eta * S + theta * grad

-- M3: k accumulators at k timescales
FOR level = 0 to k-1:
  IF pulse.is_active(level):
    S[level] = eta[level] * S[level] + theta[level] * accumulated_grad[level]
    accumulated_grad[level] = zeros()  -- reset after application
  ELSE:
    accumulated_grad[level] += grad    -- accumulate for later

-- The COMBINED momentum is a weighted sum across levels:
S_combined = sum(weight[level] * S[level] for level in 0..k)
```

## Pseudocode

```
ALGORITHM: m3_update(states: &mut [Tensor], error_buffers: &mut [Tensor],
                     grad: &Tensor, pulse: &Pulse, config: &M3Config) -> Tensor
  k = config.n_levels

  -- Step 1: Accumulate or apply per level
  FOR level = 0 to k-1:
    IF pulse.is_active(level):
      -- Active: apply accumulated error + current gradient
      combined_grad = error_buffers[level] + grad
      states[level] = config.eta[level] * states[level]
                    + config.theta[level] * combined_grad
      error_buffers[level] = zeros_like(grad)    -- reset
    ELSE:
      -- Frozen: accumulate gradient for future application
      error_buffers[level] = error_buffers[level] + grad

  -- Step 2: Combine across levels
  S_combined = zeros_like(grad)
  FOR level = 0 to k-1:
    S_combined = S_combined + config.weights[level] * states[level]

  return S_combined

FUNCTION: m3_with_muon(states, error_buffers, grad, pulse, config, k_ns) -> Tensor
  -- M3 + Newton-Schulz: the full M3 optimizer
  S_combined = m3_update(states, error_buffers, grad, pulse, config)
  X = newton_schulz_5(S_combined, k_ns)
  return X
```

## Example: k=2 Levels

```
-- Level 0: frequency 1 (updates every step)
--   Captures per-token gradient patterns
--   Fast, reactive, noisy
--   eta[0] = 0.9 (short memory)

-- Level 1: frequency 8 (updates every 8 steps)
--   Captures multi-token trends
--   Slow, stable, smooth
--   eta[1] = 0.99 (long memory)

-- Between updates, level 1 accumulates 8 gradients.
-- When it fires, it applies the ACCUMULATED gradient â€” smoother signal.
-- This is analogous to how CMS operates on model parameters,
-- but applied to the optimizer's own state.
```

## Connection to CMS

M3 is CMS applied recursively:

```
CMS on model params:   param[level] updates at frequency[level]
M3 on optimizer state:  momentum[level] updates at frequency[level]

-- CMS + M3: both model AND optimizer are multi-scale
-- This is NL IS #2 taken to its logical conclusion:
-- nested, multi-level, parallel optimization
-- at EVERY level of the system
```

## Axiom Compliance

- **NL IS #2** (nested, multi-level): k optimization levels at k frequencies
- **NL IS #8** (continuum memory): Discrete approximation of continuous temporal scales
- **NL IS #6** (optimizers are memory): Each momentum level IS a memory at its timescale
- **NL IS NOT #1** (not single-level): k levels, each independent
